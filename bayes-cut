#!/usr/bin/env python

import numpy as np
import operator
from statstools.significance import significance
from rootpy.tree import Cut
from multiprocessing import Process, cpu_count
from statstools.parallel import run_pool
from mva.categories.hadhad import (
        Category_Cuts_VBF,
        Category_Cuts_Boosted,)

from mva.categories.hadhad import PRESELECTION
from rootpy.stats import histfactory
from mva.workspace import cuts_workspace
from mva import log;
from rootpy.extern.argparse import ArgumentParser

from mva.analysis import Analysis

from mva import CONST_PARAMS, POI
parser = ArgumentParser()
parser.add_argument('--suffix', default=None)
parser.add_argument('--max-iter', type=int, default=20)
parser.add_argument('--systematics', default=True, action='store_false')
parser.add_argument('--jobs', default=-1, type=int)
args = parser.parse_args()


Boosted_Cuts = [
        ('Deta_taus', 'ditau_deta < {0}', (1.3, 1.8) ),
        ('Pt_j1'    , 'jet_0_pt > {0}', (30, 35) ),
        ('ditau_dr' , 'ditau_dr < {0}', (1, 2) ),
        ('higgspt'  , 'ditau_mmc_maxw_pt > {0}', (90, 110) ),
    ]

VBF_Cuts = [
        ('Deta_taus', 'ditau_deta < {0}', (1.3, 1.8) ),
        ('Pt_j1'    , 'jet_0_pt > {0}', (40, 100) ),
        ('Pt_j2'    , 'jet_1_pt > {0}', (30, 80) ),
        ('ditau_dr' , 'ditau_dr < {0}', (1, 2) ),
        ('Deta_jets', 'dijet_deta > {0}', (2, 6) ),
    ]

def getSignificance(category, cut_values=None):
    name = 'combined'
    analysis = Analysis(
                    year=2015,
                    systematics=False,
                    suffix=args.suffix)

    if category == 'vbf':
        Category_Cuts = Category_Cuts_VBF
        Cut_Templates = VBF_Cuts
        category_name = 'cuts_vbf_merged'
    else:
        Category_Cuts = Category_Cuts_Boosted
        Cut_Templates = Boosted_Cuts
        category_name = 'cuts_boosted_merged'

    if cut_values is not None:
        cut_values=list(cut_values)
        cut_strings, output = [], []
        for name, cutstr, bounds in Cut_Templates:
            cut_value = cut_values.pop(0)
            cut_strings.append( Cut( cutstr.format(cut_value*(bounds[1]-bounds[0]) + bounds[0]) ) )
            output.append( cutstr.format(cut_value*(bounds[1]-bounds[0]) + bounds[0]) )



        Category_Cuts.cuts = reduce( operator.__and__, cut_strings ) & PRESELECTION

    sr, cr = cuts_workspace(analysis=analysis,
                        categories=category_name,
                        masses=[125.],
                        systematics=args.systematics,
                        sideband=True,
                        cuts=None)

    measurement = histfactory.make_measurement(
        name, sr[125].values(),
        POI=POI,
        const_params=CONST_PARAMS)
    workspace = histfactory.make_workspace(measurement, name=name,
                                                   silence=True)

    sig, mu, mu_error = significance(workspace)
    if cut_values is not None:
        print "("*40
        print "\n".join(output)
        print "Significance: ", sig
        print ")"*40
    return 0 if sig != sig else -sig # Handle case where sig is nan

class Worker(Process):
    output = None
    def __init__(self, func, category, cut_values):
        super(Worker, self).__init__()
        self.cut_values = cut_values
        self.func = func
        self.category = category
    def run(self):
        sig = self.func( self.category, self.cut_values )
        self.output = sig

class ObjectiveFunction(object):
    def __init__(self, category, function ):
        self.category = category
        self.function = function

    def __call__( self, args ):
        workers = []
        log.info(args)
        for cut_values in args:
            workers.append(
                   Worker(getSignificance, self.category, cut_values )
                   )
        log.info("Number of workers in batch: {}".format(len(workers)))

        run_pool( workers, n_jobs=max(n_jobs, len(workers)) )
        output = [w.output for w in workers]
        return np.array(output)[:,np.newaxis]

import GPyOpt, GPy
from numpy.random import seed
seed(23127)

vbf_sig = getSignificance("vbf")
#boosted_sig = getSignificance("boosted")
log.info("Current sensitivity: ")
log.info("VBF: {}".format(-vbf_sig))
#log.info("Boosted: {}".format(-boosted_sig))

n_jobs = cpu_count() if args.jobs < 1 else args.jobs
log.info("Using {} CPU cores".format(n_jobs))

for cat in ('vbf', 'boosted'):

    log.info("Optimising cuts for {} region".format(cat))

    Cat_Cuts = VBF_Cuts if cat=='vbf' else Boosted_Cuts

    list_of_bounds = []
    for _, _, bounds in Cat_Cuts:
        list_of_bounds.append((0,1))
    log.info("{} parameters to optimise".format(len(list_of_bounds)))

    objective = ObjectiveFunction( cat, getSignificance )
    bo = GPyOpt.methods.BayesianOptimization(
                        f=objective,
                        bounds=list_of_bounds,
                        acquisition='EI',
                        kernel=GPy.kern.RBF(1)
                        )

    bo.run_optimization(max_iter=args.max_iter)
#                        , eps=1e-3, n_inbatch=n_jobs, n_procs=n_jobs,
#                        batch_method='predictive',
#                        acqu_optimize_method='fast_random',
#                        verbose=True)


    print "{"*30
    print bo.x_opt
    for x, (n,b) in zip(bo.x_opt, [(n,b) for n,_, b in Cat_Cuts]):
        print "{} : {}".format(n, x*(b[1]-b[0])+b[0])

    print "Significance: ", bo.fx_opt
    print "Significance: ", getSignificance(cat, bo.x_opt)
    print "VBF Initial: ", vbf_sig
#    print "Boosted Initial: ", boosted_sig

    bo.plot_acquisition(filename='plots/bayescut/{}_bo_acquisition.pdf'.format(cat))
    bo.plot_convergence(filename='plots/bayescut/{}_bo_convergence.pdf'.format(cat))
    print "}"*30
